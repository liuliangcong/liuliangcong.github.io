<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>python爬虫笔记 | 知行记</title><meta name="keywords" content="Python"><meta name="author" content="小辑轻舟"><meta name="copyright" content="小辑轻舟"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="仅记录我爬取小说、漫画、视频的过程。">
<meta property="og:type" content="article">
<meta property="og:title" content="python爬虫笔记">
<meta property="og:url" content="http://example.com/2022/09/15/python%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="知行记">
<meta property="og:description" content="仅记录我爬取小说、漫画、视频的过程。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/(3).jpg">
<meta property="article:published_time" content="2022-09-15T06:52:44.000Z">
<meta property="article:modified_time" content="2022-09-16T13:15:49.439Z">
<meta property="article:author" content="小辑轻舟">
<meta property="article:tag" content="Python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/(3).jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2022/09/15/python%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'python爬虫笔记',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-09-16 21:15:49'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">9</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">9</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/images/(3).jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">知行记</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">python爬虫笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-09-15T06:52:44.000Z" title="发表于 2022-09-15 14:52:44">2022-09-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-09-16T13:15:49.439Z" title="更新于 2022-09-16 21:15:49">2022-09-16</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>28分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="python爬虫笔记"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>网络爬虫即根据 URL 获取网页的信息。安装使用到的库：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install requests</span><br><span class="line">pip install beautifulsoup4</span><br><span class="line">pip install lxml</span><br></pre></td></tr></table></figure>

<blockquote>
<p>步骤：发起请求 -&gt; 解析数据 -&gt; 保存数据</p>
<p>Beautiful Soup中文文档：<a target="_blank" rel="noopener" href="https://beautifulsoup.readthedocs.io/zh_CN/latest/">https://beautifulsoup.readthedocs.io/zh_CN/latest/</a></p>
</blockquote>
<p><code>requests</code> 的使用：基础方法如下</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>requests.request()</td>
<td>构造—个请求，支撑以下各方法的基础方法</td>
</tr>
<tr>
<td>requests.get()</td>
<td>获取HTML网页的主要方法，对应于HTTP的GET</td>
</tr>
<tr>
<td>requests.head()</td>
<td>获取HTML网页头信息的方法，对应于HTTP的HEAD</td>
</tr>
<tr>
<td>requests.post()</td>
<td>向HTML网页提交POST请求的方法，对应于HTTP的POST</td>
</tr>
<tr>
<td>requests.put()</td>
<td>向HTML网页提交PUT请求的方法，对应于HTTP的PUT</td>
</tr>
<tr>
<td>requests.patch()</td>
<td>向HTML网页提交局部修改请求，对应于HTTP的PATCH</td>
</tr>
<tr>
<td>requests.delete()</td>
<td>向HTML页面提交删除请求，对应于HTTP的DELETE</td>
</tr>
</tbody></table>
<p>其中最常用的是 <code>requests.get()</code> 方法，它用于向服务器发起 GET 请求来获取数据。该方法需要一个参数 <code>url</code> ，表示爬取的对象。示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 爬取百度翻译</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    target = <span class="string">&quot;http://fanyi.baidu.com/&quot;</span></span><br><span class="line">    req = requests.get(url = target)</span><br><span class="line">    req.encoding = <span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">    <span class="built_in">print</span>(req.text)</span><br></pre></td></tr></table></figure>



<h2 id="实例：爬取小说"><a href="#实例：爬取小说" class="headerlink" title="实例：爬取小说"></a>实例：爬取小说</h2><h3 id="爬取网页内容"><a href="#爬取网页内容" class="headerlink" title="爬取网页内容"></a>爬取网页内容</h3><p>使用 <code>Beautiful Soup</code> 提取正文内容并清洗数据，得到章节内容。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 网址对应小说某一章内容</span></span><br><span class="line">    target = <span class="string">&#x27;http://book.zongheng.com/chapter/672340/36898237.html&#x27;</span></span><br><span class="line">    req = requests.get(url = target)</span><br><span class="line">    req.encoding = <span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">    html = req.text</span><br><span class="line">    <span class="comment"># 提取数据</span></span><br><span class="line">    bs = BeautifulSoup(html, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">    texts = bs.find(<span class="string">&#x27;div&#x27;</span>, &#123;<span class="string">&#x27;class&#x27;</span>:<span class="string">&#x27;content&#x27;</span>&#125;)</span><br><span class="line">    <span class="comment"># 此处写成print(texts.text.strip())即可</span></span><br><span class="line">    <span class="built_in">print</span>(texts.text.strip().split(<span class="string">&#x27;\xa0&#x27;</span>*<span class="number">4</span>))</span><br></pre></td></tr></table></figure>

<blockquote>
<p>解释：texts.text方法提取所有文字，strip方法去掉回车，split方法根据’\xa0’切分数据，此处有4个空格，因此’\xa0’*4（\xa0是不间断空白符<code>&amp;nbsp;</code>），即去掉多余的空格。</p>
<p>也可根据属性来查找元素：<code>html.find(attrs = &#123;&#39;class&#39;:&#39;class属性值&#39;, &#39;id&#39;:&#39;id属性值&#39;&#125;)</code></p>
</blockquote>
<h3 id="获取章节链接及章节名"><a href="#获取章节链接及章节名" class="headerlink" title="获取章节链接及章节名"></a>获取章节链接及章节名</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">server = <span class="string">&#x27;https://m.31xiaoshuo.com&#x27;</span></span><br><span class="line">target = <span class="string">&#x27;https://m.31xiaoshuo.com/12/12970/index.htm&#x27;</span></span><br><span class="line">req = requests.get(url = target)</span><br><span class="line">req.encoding = req.apparent_encoding</span><br><span class="line">bs = BeautifulSoup(req.text,<span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line"><span class="comment"># 获取章节列表</span></span><br><span class="line">chapters = bs.find(<span class="string">&#x27;ul&#x27;</span>,&#123;<span class="string">&#x27;class&#x27;</span>:<span class="string">&#x27;am-list-striped&#x27;</span>&#125;)</span><br><span class="line"><span class="comment"># 获取章节列表中所有章节链接对应的a标签</span></span><br><span class="line">chapters = chapters.find_all(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line"><span class="comment"># 遍历</span></span><br><span class="line"><span class="keyword">for</span> chapter <span class="keyword">in</span> chapters:</span><br><span class="line">    <span class="comment"># 获取a标签中的章节地址，需要补充前面的部分</span></span><br><span class="line">    url = server + chapter.get(<span class="string">&#x27;href&#x27;</span>)</span><br><span class="line">    <span class="comment"># 获取章节名</span></span><br><span class="line">    name = chapter.string</span><br><span class="line">    <span class="built_in">print</span>(url+<span class="string">&#x27; &#x27;</span>+ name)</span><br></pre></td></tr></table></figure>



<h3 id="整合代码"><a href="#整合代码" class="headerlink" title="整合代码"></a>整合代码</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_content</span>(<span class="params">target</span>):</span><br><span class="line">    req = requests.get(url = target)</span><br><span class="line">    req.encoding = <span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">    bs = BeautifulSoup(req.text, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">    texts = bs.find(<span class="string">&#x27;div&#x27;</span>, &#123;<span class="string">&#x27;id&#x27;</span>:<span class="string">&#x27;content&#x27;</span>&#125;)</span><br><span class="line">    content = texts.text.strip().split(<span class="string">&#x27;\xa0&#x27;</span>*<span class="number">4</span>)</span><br><span class="line">    <span class="keyword">return</span> content</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    server = <span class="string">&#x27;https://www.biqupai.com&#x27;</span></span><br><span class="line">    book_name = <span class="string">&#x27;全球高武.txt&#x27;</span></span><br><span class="line">    target = <span class="string">&#x27;https://www.biqupai.com/81_81336/&#x27;</span></span><br><span class="line">    req = requests.get(url = target)</span><br><span class="line">    req.encoding = <span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">    bs = BeautifulSoup(req.text, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">    chapters = bs.find(<span class="string">&#x27;div&#x27;</span>, &#123;<span class="string">&#x27;id&#x27;</span>:<span class="string">&#x27;list&#x27;</span>&#125;)</span><br><span class="line">    chapters = chapters.find_all(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">    <span class="comment"># 使用tqdm显示下载进度</span></span><br><span class="line">    <span class="keyword">for</span> chapter <span class="keyword">in</span> tqdm(chapters):</span><br><span class="line">        chapter_name = chapter.string</span><br><span class="line">        url = server + chapter.get(<span class="string">&#x27;href&#x27;</span>)</span><br><span class="line">        <span class="comment"># 获取章节内容</span></span><br><span class="line">        content = get_content(url)</span><br><span class="line">        <span class="comment"># 写入文件，追加写模式，文件不存在则创建</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(book_name,<span class="string">&#x27;a&#x27;</span>,encoding = <span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(chapter_name)</span><br><span class="line">            f.write(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">            f.write(<span class="string">&#x27;\n&#x27;</span>.join(content))</span><br><span class="line">            f.write(<span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure>



<h2 id="实例：爬取漫画"><a href="#实例：爬取漫画" class="headerlink" title="实例：爬取漫画"></a>实例：爬取漫画</h2><h3 id="获取章节名和章节链接"><a href="#获取章节名和章节链接" class="headerlink" title="获取章节名和章节链接"></a>获取章节名和章节链接</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    target = <span class="string">&#x27;https://www.dmzj.com/info/yaoshenji.html&#x27;</span></span><br><span class="line">    req = requests.get(url = target)</span><br><span class="line">    req.encoding = <span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">    bs = BeautifulSoup(req.text,<span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">    list_con_li = bs.find(<span class="string">&#x27;ul&#x27;</span>,&#123;<span class="string">&#x27;class&#x27;</span>:<span class="string">&#x27;list_con_li&#x27;</span>&#125;)</span><br><span class="line">    comic_list = list_con_li.find_all(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">    <span class="comment"># 两个列表分别存放章节名和章节链接</span></span><br><span class="line">    chapter_names = []</span><br><span class="line">    chapter_urls = []   </span><br><span class="line">    <span class="keyword">for</span> comic <span class="keyword">in</span> comic_list:</span><br><span class="line">        href = comic.get(<span class="string">&#x27;href&#x27;</span>)</span><br><span class="line">        <span class="comment"># 此处由于a标签中嵌套了span标签，因此不能直接使用.string</span></span><br><span class="line">        <span class="comment"># 使用.text获取标签中的文本</span></span><br><span class="line">        name = comic.text</span><br><span class="line">        <span class="comment"># ls.insert(i,x)：在列表ls的第i位置增加元素x，其余元素向后移</span></span><br><span class="line">        chapter_names.insert(<span class="number">0</span>,name)</span><br><span class="line">        chapter_urls.insert(<span class="number">0</span>,href)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(chapter_names)</span><br><span class="line">    <span class="built_in">print</span>(chapter_urls)  </span><br></pre></td></tr></table></figure>



<h3 id="获取漫画图片地址"><a href="#获取漫画图片地址" class="headerlink" title="获取漫画图片地址"></a>获取漫画图片地址</h3><p>右击或F12打开调试窗口，定位找到图片的链接，然后再 <code>查看网页源码</code>（在网址前加上 <code>view-source:</code> 来查看网页源码）来判断图片是否是动态加载。若网页源码中存在这些图片链接，则说明图片是 <code>非动态加载</code> 的。此时可直接爬取网页获取图片链接，然后下载图片。</p>
<p>以下部分针对 <code>动态加载</code> 的图片：</p>
<p>使用 <code>view-source:</code> 方法可以查看页面源码，但不管动态加载的内容。这里面没有图片链接，就说明图片是动态加载的。使用 JavaScript 动态加载，有两种方式：<code>外部加载</code>、<code>内部加载</code>。</p>
<ul>
<li><code>外部加载</code>：即在页面中引用JS文件</li>
<li><code>内部加载</code>：即Javascript脚本内容通过内嵌式写入html中。</li>
</ul>
<p>对于当前网站，找一个图片链接进行测试，例如：</p>
<p><code>https://images.dmzj.com/img/chapterpic/3059/14237/14395217739069.jpg</code></p>
<p>使用 <code>14395217739069</code> 在浏览器的调试页面搜索，因为一般这种动态加载，链接都是程序合成的。</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&lt;script type=<span class="string">&quot;text/javascript&quot;</span>&gt;</span><br><span class="line">        <span class="keyword">var</span> arr_img = <span class="keyword">new</span> <span class="title class_">Array</span>();</span><br><span class="line">        <span class="keyword">var</span> page = <span class="string">&#x27;&#x27;</span>;</span><br><span class="line">        <span class="built_in">eval</span>(<span class="keyword">function</span>(<span class="params">p,a,c,k,e,d</span>)&#123;e=<span class="keyword">function</span>(<span class="params">c</span>)&#123;<span class="keyword">return</span>(c&lt;a?<span class="string">&#x27;&#x27;</span>:<span class="title function_">e</span>(<span class="built_in">parseInt</span>(c/a)))+((c=c%a)&gt;<span class="number">35</span>?<span class="title class_">String</span>.<span class="title function_">fromCharCode</span>(c+<span class="number">29</span>):c.<span class="title function_">toString</span>(<span class="number">36</span>))&#125;;<span class="keyword">if</span>(!<span class="string">&#x27;&#x27;</span>.<span class="title function_">replace</span>(<span class="regexp">/^/</span>,<span class="title class_">String</span>))&#123;<span class="keyword">while</span>(c--)&#123;d[<span class="title function_">e</span>(c)]=k[c]||<span class="title function_">e</span>(c)&#125;k=[<span class="keyword">function</span>(<span class="params">e</span>)&#123;<span class="keyword">return</span> d[e]&#125;];e=<span class="keyword">function</span>(<span class="params"></span>)&#123;<span class="keyword">return</span><span class="string">&#x27;\\w+&#x27;</span>&#125;;c=<span class="number">1</span>&#125;;<span class="keyword">while</span>(c--)&#123;<span class="keyword">if</span>(k[c])&#123;p=p.<span class="title function_">replace</span>(<span class="keyword">new</span> <span class="title class_">RegExp</span>(<span class="string">&#x27;\\b&#x27;</span>+<span class="title function_">e</span>(c)+<span class="string">&#x27;\\b&#x27;</span>,<span class="string">&#x27;g&#x27;</span>),k[c])&#125;&#125;<span class="keyword">return</span> p&#125;(<span class="string">&#x27;g f=\&#x27;&#123;&quot;e&quot;:&quot;h&quot;,&quot;i&quot;:&quot;0&quot;,&quot;l&quot;:&quot;k\\/3\\/5\\/2\\/j.4\\r\\6\\/3\\/5\\/2\\/d.4\\r\\6\\/3\\/5\\/2\\/7.4\\r\\6\\/3\\/5\\/2\\/8.4\\r\\6\\/3\\/5\\/2\\/c.4\\r\\6\\/3\\/5\\/2\\/b.4\\r\\6\\/3\\/5\\/2\\/a.4\\r\\6\\/3\\/5\\/2\\/9.4\\r\\6\\/3\\/5\\/2\\/m.4\\r\\6\\/3\\/5\\/2\\/v.4\\r\\6\\/3\\/5\\/2\\/A.4\\r\\6\\/3\\/5\\/2\\/n.4\\r\\6\\/3\\/5\\/2\\/B.4\\r\\6\\/3\\/5\\/2\\/x.4\\r\\6\\/3\\/5\\/2\\/y.4&quot;,&quot;w&quot;:&quot;p&quot;,&quot;o&quot;:&quot;1&quot;,&quot;q&quot;:&quot;\\s\\u \\t\\z&quot;&#125;\&#x27;;&#x27;</span>,<span class="number">38</span>,<span class="number">38</span>,<span class="string">&#x27;||14237|chapterpic|jpg|3059|nimg|14395217891719|14395217893745|14395217913416|14395217908431|14395217904781|1439521790086|1439521788936|id|pages|var|41917|hidden|14395217739069|img|page_url|14395217918734|14395217931135|chapter_order|15|chapter_name||u7b2c01|u91cd|u8bdd|14395217923415|sum_pages|14395217940216|14395217943921|u751f|14395217926321|1439521793602&#x27;</span>.<span class="title function_">split</span>(<span class="string">&#x27;|&#x27;</span>),<span class="number">0</span>,&#123;&#125;))</span><br><span class="line">&lt;/script&gt;</span><br></pre></td></tr></table></figure>

<p>可以发现该数字就在其中，并且链接中的数字是 <code>由3059 + 长数字 + 14237组成</code>，合成数字，观察图片顺序是否正确。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://www.dmzj.com/view/yaoshenji/41917.html&#x27;</span></span><br><span class="line">req = requests.get(url = url)</span><br><span class="line">html = BeautifulSoup(req.text,<span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">script_info = html.script</span><br><span class="line"><span class="comment"># 找到所有长度为13或14的数字</span></span><br><span class="line">pics = re.findall(<span class="string">&#x27;\d&#123;13,14&#125;&#x27;</span>,<span class="built_in">str</span>(script_info))</span><br><span class="line"><span class="comment"># 找到3059</span></span><br><span class="line">chapter_qian = re.findall(<span class="string">&#x27;\|jpg\|(\d&#123;4&#125;)&#x27;</span>,<span class="built_in">str</span>(script_info))[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 找到14237</span></span><br><span class="line">chapterpic_hou = re.findall(<span class="string">&#x27;\|\|(\d&#123;5&#125;)&#x27;</span>,<span class="built_in">str</span>(script_info))[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> pic <span class="keyword">in</span> pics:</span><br><span class="line">    url = <span class="string">&#x27;https://images.dmzj.com/img/chapterpic/&#x27;</span> + chapter_qian + <span class="string">&#x27;/&#x27;</span> + chapterpic_hou + <span class="string">&#x27;/&#x27;</span> + pic + <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line">    <span class="built_in">print</span>(url)</span><br></pre></td></tr></table></figure>

<p>图片顺序有误，发现这些数字里有13位的，有14位的，并且都是以14开头的数字，猜测它 <code>末位补零后比较</code> 的结果就是图片的顺序。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://www.dmzj.com/view/yaoshenji/41917.html&#x27;</span></span><br><span class="line">req = requests.get(url = url)</span><br><span class="line">html = BeautifulSoup(req.text,<span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">script_info = html.script</span><br><span class="line">pics = re.findall(<span class="string">&#x27;\d&#123;13,14&#125;&#x27;</span>,<span class="built_in">str</span>(script_info))</span><br><span class="line"><span class="comment"># 长度为13则末尾补0</span></span><br><span class="line"><span class="keyword">for</span> idx, pic <span class="keyword">in</span> <span class="built_in">enumerate</span>(pics):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(pic) == <span class="number">13</span>:</span><br><span class="line">        pics[idx] = pic + <span class="string">&#x27;0&#x27;</span></span><br><span class="line"><span class="comment"># 排序</span></span><br><span class="line">pics = <span class="built_in">sorted</span>(pics, key=<span class="keyword">lambda</span> x:<span class="built_in">int</span>(x))</span><br><span class="line">chapterpic_qian = re.findall(<span class="string">&#x27;\|jpg\|(\d&#123;4&#125;)&#x27;</span>, <span class="built_in">str</span>(script_info))[<span class="number">0</span>]</span><br><span class="line">chapterpic_hou = re.findall(<span class="string">&#x27;\|\|(\d&#123;5&#125;)&#x27;</span>, <span class="built_in">str</span>(script_info))[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> pic <span class="keyword">in</span> pics:</span><br><span class="line">	<span class="comment"># 末尾为0则去除0，因为0是之前添加的</span></span><br><span class="line">    <span class="keyword">if</span> pic[-<span class="number">1</span>] == <span class="string">&#x27;0&#x27;</span>:</span><br><span class="line">        url = <span class="string">&#x27;https://images.dmzj.com/img/chapterpic/&#x27;</span> + chapterpic_qian + <span class="string">&#x27;/&#x27;</span> + chapterpic_hou + <span class="string">&#x27;/&#x27;</span> + pic[:-<span class="number">1</span>] + <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        url = <span class="string">&#x27;https://images.dmzj.com/img/chapterpic/&#x27;</span> + chapterpic_qian + <span class="string">&#x27;/&#x27;</span> + chapterpic_hou + <span class="string">&#x27;/&#x27;</span> + pic + <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line">    <span class="built_in">print</span>(url)</span><br></pre></td></tr></table></figure>

<p>此时得到正确顺序的图片链接。</p>
<h3 id="下载图片"><a href="#下载图片" class="headerlink" title="下载图片"></a>下载图片</h3><p>使用<code>urlretrieve</code>方法下载图片，第一个参数是 <code>下载链接</code>，第二个参数是下载后的 <code>文件保存名</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlretrieve</span><br><span class="line"></span><br><span class="line">dn_url = <span class="string">&#x27;https://images.dmzj.com/img/chapterpic/3059/14237/14395217739069.jpg&#x27;</span></span><br><span class="line">urlretrieve(dn_url,<span class="string">&#x27;1.jpg&#x27;</span>)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>特殊情况：</p>
<p>出现了HTTP Error，错误代码是 <code>403</code>。403表示资源不可用，这是又是一种 <code>反爬虫</code> 手段。对应图片的真实地址，在浏览器可能无法直接打开，或者能打开，但是一刷新就又不能打开了！如果打开章节页面后，再打开这个图片链接就又能看到图片了。这就是一种 <code>通过Referer的反爬虫手段</code>。Referer可以理解为来路，先打开章节URL链接，再打开图片链接。打开图片的时候，Referer的信息里保存的是章节URL，因此爬取时需要 <code>添加头部信息</code>。</p>
<p>使用 <code>closing</code> 方法可以设置Headers信息，这个Headers信息里保存Referer来路，即章节的链接。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> contextlib <span class="keyword">import</span> closing</span><br><span class="line"></span><br><span class="line">download_header = &#123;</span><br><span class="line">    <span class="string">&#x27;Referer&#x27;</span>: <span class="string">&#x27;https://www.dmzj.com/view/yaoshenji/41917.html&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">dn_url = <span class="string">&#x27;https://images.dmzj.com/img/chapterpic/3059/14237/14395217739069.jpg&#x27;</span></span><br><span class="line"><span class="keyword">with</span> closing(requests.get(dn_url, headers = download_header, stream = <span class="literal">True</span>)) <span class="keyword">as</span> response:</span><br><span class="line">    chunk_size = <span class="number">1024</span>  </span><br><span class="line">    content_size = <span class="built_in">int</span>(response.headers[<span class="string">&#x27;content-length&#x27;</span>])  </span><br><span class="line">    <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;文件大小:%0.2f KB&#x27;</span> % (content_size / chunk_size))</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;1.jpg&#x27;</span>, <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> file:  </span><br><span class="line">            <span class="keyword">for</span> data <span class="keyword">in</span> response.iter_content(chunk_size=chunk_size):  </span><br><span class="line">                file.write(data)  </span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;链接异常&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;下载完成！&#x27;</span>)</span><br></pre></td></tr></table></figure>



<h3 id="整合代码-1"><a href="#整合代码-1" class="headerlink" title="整合代码"></a>整合代码</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> contextlib <span class="keyword">import</span> closing</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建保存目录</span></span><br><span class="line">save_dir = <span class="string">&#x27;妖神记&#x27;</span></span><br><span class="line"><span class="keyword">if</span> save_dir <span class="keyword">not</span> <span class="keyword">in</span> os.listdir(<span class="string">&#x27;./&#x27;</span>):</span><br><span class="line">    <span class="comment"># 列出当前执行路径目录下的文件</span></span><br><span class="line">    os.mkdir(save_dir)</span><br><span class="line"></span><br><span class="line">target_url = <span class="string">&quot;https://www.dmzj.com/info/yaoshenji.html&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取动漫章节链接和章节名</span></span><br><span class="line">r = requests.get(url = target_url)</span><br><span class="line">bs = BeautifulSoup(r.text, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">list_con_li = bs.find(<span class="string">&#x27;ul&#x27;</span>, class_=<span class="string">&quot;list_con_li&quot;</span>)</span><br><span class="line">cartoon_list = list_con_li.find_all(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">chapter_names = []</span><br><span class="line">chapter_urls = []</span><br><span class="line"><span class="keyword">for</span> cartoon <span class="keyword">in</span> cartoon_list:</span><br><span class="line">    href = cartoon.get(<span class="string">&#x27;href&#x27;</span>)</span><br><span class="line">    name = cartoon.text</span><br><span class="line">    chapter_names.insert(<span class="number">0</span>, name)</span><br><span class="line">    chapter_urls.insert(<span class="number">0</span>, href)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载漫画 </span></span><br><span class="line"><span class="keyword">for</span> i, url <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm(chapter_urls)):</span><br><span class="line">    download_header = &#123;</span><br><span class="line">        <span class="string">&#x27;Referer&#x27;</span>: url</span><br><span class="line">    &#125;</span><br><span class="line">    name = chapter_names[i]</span><br><span class="line">    <span class="comment"># 去掉.</span></span><br><span class="line">    <span class="keyword">while</span> <span class="string">&#x27;.&#x27;</span> <span class="keyword">in</span> name:</span><br><span class="line">        name = name.replace(<span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    chapter_save_dir = os.path.join(save_dir, name)</span><br><span class="line">    <span class="keyword">if</span> name <span class="keyword">not</span> <span class="keyword">in</span> os.listdir(save_dir):</span><br><span class="line">        os.mkdir(chapter_save_dir)</span><br><span class="line">        r = requests.get(url = url)</span><br><span class="line">        html = BeautifulSoup(r.text, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">        script_info = html.script</span><br><span class="line">        pics = re.findall(<span class="string">&#x27;\d&#123;13,14&#125;&#x27;</span>, <span class="built_in">str</span>(script_info))</span><br><span class="line">        <span class="keyword">for</span> j, pic <span class="keyword">in</span> <span class="built_in">enumerate</span>(pics):</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(pic) == <span class="number">13</span>:</span><br><span class="line">                pics[j] = pic + <span class="string">&#x27;0&#x27;</span></span><br><span class="line">        pics = <span class="built_in">sorted</span>(pics, key=<span class="keyword">lambda</span> x:<span class="built_in">int</span>(x))</span><br><span class="line">        chapterpic_hou = re.findall(<span class="string">&#x27;\|(\d&#123;5&#125;)\|&#x27;</span>, <span class="built_in">str</span>(script_info))[<span class="number">0</span>]</span><br><span class="line">        chapterpic_qian = re.findall(<span class="string">&#x27;\|(\d&#123;4&#125;)\|&#x27;</span>, <span class="built_in">str</span>(script_info))[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> idx, pic <span class="keyword">in</span> <span class="built_in">enumerate</span>(pics):</span><br><span class="line">            <span class="keyword">if</span> pic[-<span class="number">1</span>] == <span class="string">&#x27;0&#x27;</span>:</span><br><span class="line">                url = <span class="string">&#x27;https://images.dmzj.com/img/chapterpic/&#x27;</span> + chapterpic_qian + <span class="string">&#x27;/&#x27;</span> + chapterpic_hou + <span class="string">&#x27;/&#x27;</span> + pic[:-<span class="number">1</span>] + <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                url = <span class="string">&#x27;https://images.dmzj.com/img/chapterpic/&#x27;</span> + chapterpic_qian + <span class="string">&#x27;/&#x27;</span> + chapterpic_hou + <span class="string">&#x27;/&#x27;</span> + pic + <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line">            pic_name = <span class="string">&#x27;%03d.jpg&#x27;</span> % (idx + <span class="number">1</span>)</span><br><span class="line">            pic_save_path = os.path.join(chapter_save_dir, pic_name)</span><br><span class="line">            <span class="keyword">with</span> closing(requests.get(url, headers = download_header, stream = <span class="literal">True</span>)) <span class="keyword">as</span> response:  </span><br><span class="line">                chunk_size = <span class="number">1024</span>  </span><br><span class="line">                content_size = <span class="built_in">int</span>(response.headers[<span class="string">&#x27;content-length&#x27;</span>])  </span><br><span class="line">                <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">                    <span class="keyword">with</span> <span class="built_in">open</span>(pic_save_path, <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> file:  </span><br><span class="line">                        <span class="keyword">for</span> data <span class="keyword">in</span> response.iter_content(chunk_size=chunk_size):  </span><br><span class="line">                            file.write(data)  </span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&#x27;链接异常&#x27;</span>)</span><br><span class="line">        time.sleep(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>



<h2 id="实例：爬取视频"><a href="#实例：爬取视频" class="headerlink" title="实例：爬取视频"></a>实例：爬取视频</h2><h3 id="查找资源"><a href="#查找资源" class="headerlink" title="查找资源"></a>查找资源</h3><p>抓取 <code>搜索的请求包</code>：输入要搜索的内容，然后打开并清理 <code>Network</code>，点击搜索按钮。此时 Network 里第一个弹出的就是搜索的请求包。它是一个 POST 请求，即给服务器发送数据。</p>
<p>找到搜索结果对应的标签进行爬取，爬取时需要根据浏览器的抓包结果，添加 <code>data信息</code> 和 <code>Headers信息</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">search_keyword = <span class="string">&#x27;鬼灭之刃第二季&#x27;</span></span><br><span class="line">search_url = <span class="string">&#x27;http://www.yinghuaz.com/search/-------------.html&#x27;</span></span><br><span class="line"><span class="comment"># 存在参数的搜索页面</span></span><br><span class="line"><span class="comment"># http://www.jisudhw.com/index.php?m=vod-search</span></span><br><span class="line"><span class="comment"># serach_params = &#123;</span></span><br><span class="line"><span class="comment">#    &#x27;m&#x27;: &#x27;vod-search&#x27;</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br><span class="line">serach_headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Referer&#x27;</span>: <span class="string">&#x27;http://www.yinghuaz.com/search/-------------.html&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Origin&#x27;</span>: <span class="string">&#x27;http://www.yinghuaz.com&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Host&#x27;</span>: <span class="string">&#x27;www.yinghuaz.com&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">serach_datas = &#123;</span><br><span class="line">    <span class="string">&#x27;wd&#x27;</span>: search_keyword,</span><br><span class="line">    <span class="string">&#x27;submit&#x27;</span>: <span class="string">&#x27;search&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">r = requests.post(url=search_url, headers=serach_headers, data=serach_datas)</span><br><span class="line"><span class="comment"># r = requests.post(url=search_url, params=serach_params, headers=serach_headers, data=serach_datas)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取搜索结果</span></span><br><span class="line">r.encoding = <span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">server = <span class="string">&#x27;http://www.yinghuaz.com&#x27;</span></span><br><span class="line">search_html = BeautifulSoup(r.text, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">search_lists = search_html.find(<span class="string">&#x27;div&#x27;</span>, class_=<span class="string">&#x27;imgs&#x27;</span>)</span><br><span class="line">url = server + search_lists.find(<span class="string">&#x27;p&#x27;</span>).find(<span class="string">&#x27;a&#x27;</span>).get(<span class="string">&#x27;href&#x27;</span>)</span><br><span class="line">name = search_lists.find(<span class="string">&#x27;p&#x27;</span>).find(<span class="string">&#x27;a&#x27;</span>).string</span><br><span class="line"><span class="built_in">print</span>(url + <span class="string">&#x27;&#x27;</span> +name)</span><br></pre></td></tr></table></figure>



<h3 id="获取每集链接"><a href="#获取每集链接" class="headerlink" title="获取每集链接"></a>获取每集链接</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">url=<span class="string">&#x27;http://www.yinghuaz.com/view/2391.html鬼灭之刃第二季&#x27;</span></span><br><span class="line">req = requests.get(url = url)</span><br><span class="line">req.encoding = <span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">bs = BeautifulSoup(req.text, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">bs = bs.find(<span class="string">&quot;div&quot;</span>,&#123;<span class="string">&#x27;id&#x27;</span>:<span class="string">&#x27;play_1&#x27;</span>&#125;)</span><br><span class="line">episodes = bs.find_all(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">num = <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> episode <span class="keyword">in</span> episodes:</span><br><span class="line">    url = server + episode.get(<span class="string">&#x27;href&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;第%0d集：&#x27;</span>%num + <span class="string">&#x27; &#x27;</span> + url)</span><br><span class="line">    num+=<span class="number">1</span></span><br></pre></td></tr></table></figure>



<h3 id="获取每集视频对应的链接"><a href="#获取每集视频对应的链接" class="headerlink" title="获取每集视频对应的链接"></a>获取每集视频对应的链接</h3><p>查找视频对应的链接，发现每一集视频都在id为WANG的iframe标签中，但通过view-source查看源码时发现iframe不存在，但id为playbox的div中也存在视频链接，只是缺少前面一部分，补齐即可。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> episode <span class="keyword">in</span> episodes:</span><br><span class="line">    url = server + episode.get(<span class="string">&#x27;href&#x27;</span>)</span><br><span class="line">    req = requests.get(url = url)</span><br><span class="line">    bs = BeautifulSoup(req.text,<span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">    content = <span class="string">&#x27;https://jx.wpng.cc/m.php?url=&#x27;</span> + bs.find(<span class="string">&#x27;div&#x27;</span>,&#123;<span class="string">&#x27;id&#x27;</span>:<span class="string">&#x27;playbox&#x27;</span>&#125;).get(<span class="string">&#x27;data-vid&#x27;</span>)</span><br><span class="line">	<span class="comment"># 第11集视频链接少了两位&#x27;u8&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(content)==<span class="number">103</span>:</span><br><span class="line">        content+=<span class="string">&#x27;u8&#x27;</span></span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&#x27;视频&#x27;</span> + <span class="built_in">str</span>(num) + <span class="string">&#x27;链接：&#x27;</span> + <span class="string">&#x27; &#x27;</span> + content)</span><br><span class="line">    num+=<span class="number">1</span></span><br></pre></td></tr></table></figure>



<h3 id="下载视频"><a href="#下载视频" class="headerlink" title="下载视频"></a>下载视频</h3><p>视频下载，存在两者情况：</p>
<blockquote>
<p>链接以 <code>mp4、mkv、rmvb</code> 格式为后缀，使用图片下载方法即可。</p>
<p>链接以 <code>m3u8</code> 为后缀结尾的链接，这种格式的视频分段存储，由一个个ts片段组成。可以把 m3u8 文件理解为链表，每个 ts 视频片段文件都有下一个时序的 ts 视频片段的地址。</p>
</blockquote>
<p>下载时使用 <code>FFmpeg（多媒体视频处理工具）</code>。安装FFmpeg：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install ffmpy3</span><br></pre></td></tr></table></figure>

<p>下载 <code>m3u8</code> 类型视频，方式一：<code>命令行</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ffmpeg -i <span class="string">&quot;http://youku.com-youku.net/20180614/11920_4c9e1cc1/index.m3u8&quot;</span> <span class="string">&quot;第001集.mp4&quot;</span></span><br></pre></td></tr></table></figure>

<p>方式二：<code>Python代码</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> ffmpy3</span><br><span class="line"></span><br><span class="line">exe = <span class="string">r&#x27;F:\学习\Python\ffmpeg-master-latest-win64-lgpl-shared\bin\ffmpeg.exe&#x27;</span></span><br><span class="line">ff = ffmpy3.FFmpeg(executable=exe, inputs=&#123;<span class="string">&#x27;http://youku.com-youku.net/20180614/11920_4c9e1cc1/index.m3u8&#x27;</span>:<span class="literal">None</span>&#125;,outputs=&#123;<span class="string">&#x27;第1集.mp4&#x27;</span>:<span class="literal">None</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(ff.cmd)</span><br><span class="line">ff.run()</span><br></pre></td></tr></table></figure>

<blockquote>
<p>FFmpeg 自动整合 ts 分段视频，并保存为 mp4 格式的视频。</p>
<p>FFmpeg.exe下载地址：<a target="_blank" rel="noopener" href="https://github.com/BtbN/FFmpeg-Builds/releases">https://github.com/BtbN/FFmpeg-Builds/releases</a></p>
</blockquote>
<p>下载 <code>MP4</code> 类型的视频</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> ffmpy3</span><br><span class="line"></span><br><span class="line">exe = <span class="string">r&#x27;F:\学习\Python\ffmpeg-master-latest-win64-lgpl-shared\bin\ffmpeg.exe&#x27;</span></span><br><span class="line">ff = ffmpy3.FFmpeg(executable=exe, inputs=&#123;<span class="string">&#x27;https://ali-ad.a.yximgs.com/bs2/ad-creative-center-temp/1b8e681bf6284f8dbb5bd7eb56eb27c7.mp4&#x27;</span>:<span class="literal">None</span>&#125;,outputs=&#123;<span class="string">&#x27;第2集.mp4&#x27;</span>:<span class="literal">None</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(ff.cmd)</span><br><span class="line">ff.run()</span><br></pre></td></tr></table></figure>



<h3 id="整合代码-2"><a href="#整合代码-2" class="headerlink" title="整合代码"></a>整合代码</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> ffmpy3</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> multiprocessing.dummy <span class="keyword">import</span> Pool <span class="keyword">as</span> ThreadPool</span><br><span class="line"></span><br><span class="line">search_keyword = <span class="string">&#x27;鬼灭之刃第二季&#x27;</span></span><br><span class="line">search_url = <span class="string">&#x27;http://www.yinghuaz.com/search/-------------.html&#x27;</span></span><br><span class="line">serach_headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Referer&#x27;</span>: <span class="string">&#x27;http://www.yinghuaz.com/search/-------------.html&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Origin&#x27;</span>: <span class="string">&#x27;http://www.yinghuaz.com&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Host&#x27;</span>: <span class="string">&#x27;www.yinghuaz.com&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">serach_datas = &#123;</span><br><span class="line">    <span class="string">&#x27;wd&#x27;</span>: search_keyword,</span><br><span class="line">    <span class="string">&#x27;submit&#x27;</span>: <span class="string">&#x27;search&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">r = requests.post(url=search_url, headers=serach_headers, data=serach_datas)</span><br><span class="line">r.encoding = <span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">server = <span class="string">&#x27;http://www.yinghuaz.com&#x27;</span></span><br><span class="line">search_html = BeautifulSoup(r.text, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">search_lists = search_html.find(<span class="string">&#x27;div&#x27;</span>, class_=<span class="string">&#x27;imgs&#x27;</span>)</span><br><span class="line">url = server + search_lists.find(<span class="string">&#x27;p&#x27;</span>).find(<span class="string">&#x27;a&#x27;</span>).get(<span class="string">&#x27;href&#x27;</span>)</span><br><span class="line">name = search_lists.find(<span class="string">&#x27;p&#x27;</span>).find(<span class="string">&#x27;a&#x27;</span>).string</span><br><span class="line"><span class="keyword">if</span> name <span class="keyword">not</span> <span class="keyword">in</span> os.listdir(<span class="string">&#x27;./&#x27;</span>):</span><br><span class="line">    os.mkdir(name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取每一集的url</span></span><br><span class="line">req = requests.get(url = url)</span><br><span class="line">req.encoding = <span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">bs = BeautifulSoup(req.text, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">bs = bs.find(<span class="string">&quot;div&quot;</span>,&#123;<span class="string">&#x27;id&#x27;</span>:<span class="string">&#x27;play_1&#x27;</span>&#125;)</span><br><span class="line">episodes = bs.find_all(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">num = <span class="number">1</span></span><br><span class="line">serach_res = &#123;&#125;</span><br><span class="line">exe = <span class="string">r&#x27;F:\学习\Python\ffmpeg-master-latest-win64-lgpl-shared\bin\ffmpeg.exe&#x27;</span></span><br><span class="line"><span class="keyword">for</span> episode <span class="keyword">in</span> episodes:</span><br><span class="line">    <span class="comment"># 每一集的url</span></span><br><span class="line">    url = server + episode.get(<span class="string">&#x27;href&#x27;</span>)</span><br><span class="line">    req = requests.get(url = url)</span><br><span class="line">    bs = BeautifulSoup(req.text,<span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">    <span class="comment"># 每一集视频的url</span></span><br><span class="line">    url = <span class="string">&#x27;https://jx.wpng.cc/m.php?url=&#x27;</span> + bs.find(<span class="string">&#x27;div&#x27;</span>,&#123;<span class="string">&#x27;id&#x27;</span>:<span class="string">&#x27;playbox&#x27;</span>&#125;).get(<span class="string">&#x27;data-vid&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(url)==<span class="number">103</span>:</span><br><span class="line">        url+=<span class="string">&#x27;u8&#x27;</span></span><br><span class="line">    serach_res[url] = num</span><br><span class="line">    num+=<span class="number">1</span></span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">downVideo</span>(<span class="params">url</span>):</span><br><span class="line">    num = serach_res[url]</span><br><span class="line">    name = os.path.join(video_dir,<span class="string">&#x27;第%02d集.mp4&#x27;</span> %num)</span><br><span class="line">    ff = ffmpy3.FFmpeg(executable=exe, inputs=&#123;url:<span class="literal">None</span>&#125;,outputs=&#123;name:<span class="literal">None</span>&#125;)</span><br><span class="line">    <span class="built_in">print</span>(ff.cmd)</span><br><span class="line">    ff.run()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开8个线程    </span></span><br><span class="line">pool = ThreadPool(<span class="number">8</span>)</span><br><span class="line">results = pool.<span class="built_in">map</span>(downVideo,serach_res.keys())</span><br><span class="line">pool.close()</span><br><span class="line">pool.join()</span><br></pre></td></tr></table></figure>



<h3 id="特殊情况——非真实链接"><a href="#特殊情况——非真实链接" class="headerlink" title="特殊情况——非真实链接"></a>特殊情况——非真实链接</h3><p>当我们得到一个m3u8形式的视频链接时，需要测试该链接是否是 <code>真实地址</code>。</p>
<p>运行下方代码测试 <code>https://mgtv.sd-play.com/20211006/UiC7BSo5/index.m3u8</code>，从结果可以看出这是一个嵌套的地址。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> m3u8</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.60 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">playlist = m3u8.load(uri=<span class="string">&#x27;https://mgtv.sd-play.com/20211006/UiC7BSo5/index.m3u8&#x27;</span>, headers=headers)</span><br><span class="line"><span class="built_in">print</span>(playlist.data)</span><br></pre></td></tr></table></figure>

<p>解析真实地址，运行下方代码，结果为：</p>
<p><code>https://mgtv.sd-play.com/20211006/UiC7BSo5/1000kb/hls/index.m3u8</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> m3u8</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.60 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_real_url</span>(<span class="params">url</span>):</span><br><span class="line">    playlist = m3u8.load(uri=url, headers=headers)</span><br><span class="line">    <span class="keyword">return</span> playlist.playlists[<span class="number">0</span>].absolute_uri</span><br><span class="line"></span><br><span class="line">real_url = get_real_url(<span class="string">&#x27;https://mgtv.sd-play.com/20211006/UiC7BSo5/index.m3u8&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(real_url)</span><br></pre></td></tr></table></figure>

<p>解析真实地址的加密key，运行代码后可以看到密钥下载地址和加密类型，结果为：</p>
<p><code>https://mgtv.shanshanku.com/20211006/UiC7BSo5/1000kb/hls/key.key AES-128 None</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">playlist = m3u8.load(uri=real_url, headers=headers)</span><br><span class="line">key = playlist.keys[-<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(key.uri, key.method, key.iv)</span><br></pre></td></tr></table></figure>

<p>使用request下载密钥，运行结果为：</p>
<p><code>b&#39;888288aee8b98ae9&#39;</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(playlist.keys[<span class="number">0</span>].uri, headers=headers)</span><br><span class="line">key = r.content</span><br><span class="line"><span class="built_in">print</span>(key)</span><br></pre></td></tr></table></figure>

<p><code>单线程</code> 直接下载视频，整合代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> m3u8</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> Crypto.Cipher <span class="keyword">import</span> AES</span><br><span class="line"><span class="keyword">from</span> Crypto.Util.Padding <span class="keyword">import</span> pad</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.60 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_real_url</span>(<span class="params">url</span>):</span><br><span class="line">    playlist = m3u8.load(uri=url, headers=headers)</span><br><span class="line">    <span class="keyword">return</span> playlist.playlists[<span class="number">0</span>].absolute_uri</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">AESDecrypt</span>(<span class="params">cipher_text, key, iv</span>):</span><br><span class="line">    cipher_text = pad(data_to_pad=cipher_text, block_size=AES.block_size)</span><br><span class="line">    aes = AES.new(key=key, mode=AES.MODE_CBC, iv=key)</span><br><span class="line">    cipher_text = aes.decrypt(cipher_text)</span><br><span class="line">    <span class="keyword">return</span> cipher_text</span><br><span class="line"></span><br><span class="line">real_url = get_real_url(<span class="string">&#x27;https://mgtv.sd-play.com/20211006/UiC7BSo5/index.m3u8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">playlist = m3u8.load(uri=real_url, headers=headers)</span><br><span class="line">key = playlist.keys[-<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(key.uri, key.method, key.iv)</span><br><span class="line"></span><br><span class="line">r = requests.get(playlist.keys[<span class="number">0</span>].uri, headers=headers)</span><br><span class="line">key = r.content</span><br><span class="line"><span class="built_in">print</span>(key)</span><br><span class="line"></span><br><span class="line">n = <span class="built_in">len</span>(playlist.segments)</span><br><span class="line"><span class="comment"># playlist.segments中存储的时各个ts片段</span></span><br><span class="line">size = <span class="number">0</span></span><br><span class="line">start = time.time()</span><br><span class="line"><span class="keyword">for</span> i, seg <span class="keyword">in</span> <span class="built_in">enumerate</span>(playlist.segments, <span class="number">1</span>):</span><br><span class="line">    r = requests.get(seg.absolute_uri, headers=headers)</span><br><span class="line">    data = r.content</span><br><span class="line">    data = AESDecrypt(data, key=key, iv=key)</span><br><span class="line">    size += <span class="built_in">len</span>(data)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;reusult.mp4&quot;</span>, <span class="string">&quot;ab&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\r下载进度(<span class="subst">&#123;i&#125;</span>/<span class="subst">&#123;n&#125;</span>)，已下载：<span class="subst">&#123;size/<span class="number">1024</span>/<span class="number">1024</span>:<span class="number">.2</span>f&#125;</span>MB，下载已耗时：<span class="subst">&#123;time.time()-start:<span class="number">.2</span>f&#125;</span>s&quot;</span>, end=<span class="string">&quot; &quot;</span>)</span><br></pre></td></tr></table></figure>

<p>封装函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> Crypto.Util.Padding <span class="keyword">import</span> pad</span><br><span class="line"><span class="keyword">from</span> Crypto.Cipher <span class="keyword">import</span> AES</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> m3u8</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_real_url</span>(<span class="params">url</span>):</span><br><span class="line">    playlist = m3u8.load(uri=url, headers=headers)</span><br><span class="line">    <span class="keyword">return</span> playlist.playlists[<span class="number">0</span>].absolute_uri</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">AESDecrypt</span>(<span class="params">cipher_text, key, iv</span>):</span><br><span class="line">    cipher_text = pad(data_to_pad=cipher_text, block_size=AES.block_size)</span><br><span class="line">    aes = AES.new(key=key, mode=AES.MODE_CBC, iv=key)</span><br><span class="line">    cipher_text = aes.decrypt(cipher_text)</span><br><span class="line">    <span class="keyword">return</span> cipher_text</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download_m3u8_video</span>(<span class="params">url, save_name</span>):</span><br><span class="line">    real_url = get_real_url(url)</span><br><span class="line">    playlist = m3u8.load(uri=real_url, headers=headers)</span><br><span class="line">    key = requests.get(playlist.keys[-<span class="number">1</span>].uri, headers=headers).content</span><br><span class="line"></span><br><span class="line">    n = <span class="built_in">len</span>(playlist.segments)</span><br><span class="line">    size = <span class="number">0</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    <span class="keyword">for</span> i, seg <span class="keyword">in</span> <span class="built_in">enumerate</span>(playlist.segments, <span class="number">1</span>):</span><br><span class="line">        r = requests.get(seg.absolute_uri, headers=headers)</span><br><span class="line">        data = r.content</span><br><span class="line">        data = AESDecrypt(data, key=key, iv=key)</span><br><span class="line">        size += <span class="built_in">len</span>(data)</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(save_name, <span class="string">&quot;ab&quot;</span> <span class="keyword">if</span> i != <span class="number">1</span> <span class="keyword">else</span> <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(data)</span><br><span class="line">        <span class="built_in">print</span>(</span><br><span class="line">            <span class="string">f&quot;\r下载进度(<span class="subst">&#123;i&#125;</span>/<span class="subst">&#123;n&#125;</span>)，已下载：<span class="subst">&#123;size/<span class="number">1024</span>/<span class="number">1024</span>:<span class="number">.2</span>f&#125;</span>MB，下载已耗时：<span class="subst">&#123;time.time()-start:<span class="number">.2</span>f&#125;</span>s&quot;</span>, end=<span class="string">&quot; &quot;</span>)</span><br><span class="line"></span><br><span class="line">download_m3u8_video(<span class="string">&#x27;https://vod8.wenshibaowenbei.com/20210628/g4yNLlI7/index.m3u8&#x27;</span>, <span class="string">&#x27;走进家门.mp4&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><code>多线程</code> 改造：对于多线程，由于下载的文件可能出现间断，所以不能直接追加到目标视频中，可以先下载下来，最后统一合并并删除。</p>
<p>先创建ts视频下载的方法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download_ts</span>(<span class="params">url, key, i</span>):</span><br><span class="line">    r = requests.get(url, headers=headers)</span><br><span class="line">    data = r.content</span><br><span class="line">    data = AESDecrypt(data, key=key, iv=key)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">f&quot;tmp/<span class="subst">&#123;i:<span class="number">0</span>&gt;5d&#125;</span>.ts&quot;</span>, <span class="string">&quot;ab&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\r<span class="subst">&#123;i:<span class="number">0</span>&gt;5d&#125;</span>.ts已下载&quot;</span>, end=<span class="string">&quot;  &quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&quot;tmp&quot;</span>):</span><br><span class="line">    os.mkdir(<span class="string">&#x27;tmp&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>任意下载一个片段测试，检查该片段是否可以正常播放</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> m3u8</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_real_url</span>(<span class="params">url</span>):</span><br><span class="line">    playlist = m3u8.load(uri=url, headers=headers)</span><br><span class="line">    <span class="keyword">return</span> playlist.playlists[<span class="number">0</span>].absolute_uri</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.60 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">real_url = get_real_url(</span><br><span class="line">    <span class="string">&#x27;https://mgtv.sd-play.com/20211006/UiC7BSo5/index.m3u8&#x27;</span>)</span><br><span class="line">playlist = m3u8.load(uri=real_url, headers=headers)</span><br><span class="line">key = requests.get(playlist.keys[-<span class="number">1</span>].uri, headers=headers).content</span><br><span class="line"></span><br><span class="line">download_ts(playlist.segments[<span class="number">0</span>].absolute_uri, key, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>10个线程同时一起下载</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> concurrent.futures <span class="keyword">import</span> ThreadPoolExecutor</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> ThreadPoolExecutor(max_workers=<span class="number">10</span>) <span class="keyword">as</span> pool:</span><br><span class="line">    <span class="keyword">for</span> i, seg <span class="keyword">in</span> <span class="built_in">enumerate</span>(playlist.segments):</span><br><span class="line">        pool.submit(download_ts, seg.absolute_uri, key, i)</span><br></pre></td></tr></table></figure>

<p>实现文件的合并和ts临时文件清除</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;video.mp4&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> fw:</span><br><span class="line">    files = glob.glob(<span class="string">&#x27;tmp/*.ts&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(file, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> fr:</span><br><span class="line">            fw.write(fr.read())</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;\r<span class="subst">&#123;file&#125;</span>已合并!总数:<span class="subst">&#123;<span class="built_in">len</span>(files)&#125;</span>&#x27;</span>, end=<span class="string">&quot;     &quot;</span>)</span><br><span class="line">        os.remove(file)</span><br></pre></td></tr></table></figure>

<p>完整代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">from</span> concurrent.futures <span class="keyword">import</span> ThreadPoolExecutor</span><br><span class="line"><span class="keyword">import</span> m3u8</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> Crypto.Util.Padding <span class="keyword">import</span> pad</span><br><span class="line"><span class="keyword">from</span> Crypto.Cipher <span class="keyword">import</span> AES</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download_ts</span>(<span class="params">url, key, i</span>):</span><br><span class="line">    r = requests.get(url, headers=headers)</span><br><span class="line">    data = r.content</span><br><span class="line">    data = AESDecrypt(data, key=key, iv=key)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">f&quot;tmp/<span class="subst">&#123;i:<span class="number">0</span>&gt;5d&#125;</span>.ts&quot;</span>, <span class="string">&quot;ab&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\r<span class="subst">&#123;i:<span class="number">0</span>&gt;5d&#125;</span>.ts已下载&quot;</span>, end=<span class="string">&quot;  &quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_real_url</span>(<span class="params">url</span>):</span><br><span class="line">    playlist = m3u8.load(uri=url, headers=headers)</span><br><span class="line">    <span class="keyword">return</span> playlist.playlists[<span class="number">0</span>].absolute_uri</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">AESDecrypt</span>(<span class="params">cipher_text, key, iv</span>):</span><br><span class="line">    cipher_text = pad(data_to_pad=cipher_text, block_size=AES.block_size)</span><br><span class="line">    aes = AES.new(key=key, mode=AES.MODE_CBC, iv=key)</span><br><span class="line">    cipher_text = aes.decrypt(cipher_text)</span><br><span class="line">    <span class="keyword">return</span> cipher_text</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download_m3u8_video</span>(<span class="params">url, save_name, max_workers=<span class="number">10</span></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&quot;tmp&quot;</span>):</span><br><span class="line">        os.mkdir(<span class="string">&#x27;tmp&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    real_url = get_real_url(url)</span><br><span class="line">    playlist = m3u8.load(uri=real_url, headers=headers)</span><br><span class="line">    key = requests.get(playlist.keys[-<span class="number">1</span>].uri, headers=headers).content</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> ThreadPoolExecutor(max_workers=max_workers) <span class="keyword">as</span> pool:</span><br><span class="line">        <span class="keyword">for</span> i, seg <span class="keyword">in</span> <span class="built_in">enumerate</span>(playlist.segments):</span><br><span class="line">            pool.submit(download_ts, seg.absolute_uri, key, i)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(save_name, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> fw:</span><br><span class="line">        files = glob.glob(<span class="string">&#x27;tmp/*.ts&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(file, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> fr:</span><br><span class="line">                fw.write(fr.read())</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&#x27;\r<span class="subst">&#123;file&#125;</span>已合并!总数:<span class="subst">&#123;<span class="built_in">len</span>(files)&#125;</span>&#x27;</span>, end=<span class="string">&quot;     &quot;</span>)</span><br><span class="line">            os.remove(file)</span><br><span class="line"></span><br><span class="line">download_m3u8_video(<span class="string">&#x27;https://vod8.wenshibaowenbei.com/20210628/g4yNLlI7/index.m3u8&#x27;</span>, <span class="string">&#x27;走进家门.mp4&#x27;</span>)</span><br></pre></td></tr></table></figure>



<h2 id="自动化测试工具-Selenium"><a href="#自动化测试工具-Selenium" class="headerlink" title="自动化测试工具 Selenium"></a>自动化测试工具 Selenium</h2><h3 id="安装Selenium及浏览器驱动"><a href="#安装Selenium及浏览器驱动" class="headerlink" title="安装Selenium及浏览器驱动"></a>安装Selenium及浏览器驱动</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install selenium</span><br></pre></td></tr></table></figure>

<blockquote>
<p>浏览器驱动下载地址：<a target="_blank" rel="noopener" href="http://chromedriver.storage.googleapis.com/index.html">http://chromedriver.storage.googleapis.com/index.html</a></p>
<p>注意：安装时需要注意浏览器 <code>版本对应</code>。</p>
</blockquote>
<h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 打开www.python.org官网，找到name属性为q的搜索框，输入pycon并点击搜索。</span></span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.keys <span class="keyword">import</span> Keys</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    driver = webdriver.Chrome(<span class="string">&quot;F:\学习\Python\chromedriver.exe&quot;</span>)</span><br><span class="line">    driver.get(<span class="string">&quot;https://www.python.org&quot;</span>)</span><br><span class="line">    <span class="keyword">assert</span> <span class="string">&quot;Python&quot;</span> <span class="keyword">in</span> driver.title</span><br><span class="line">    elem = driver.find_element_by_name(<span class="string">&quot;q&quot;</span>)</span><br><span class="line">    elem.send_keys(<span class="string">&quot;pycon&quot;</span>)</span><br><span class="line">    elem.send_keys(Keys.RETURN)</span><br><span class="line">    <span class="built_in">print</span>(driver.page_source)</span><br></pre></td></tr></table></figure>



<h3 id="find-element-by"><a href="#find-element-by" class="headerlink" title="find_element_by_*"></a>find_element_by_*</h3><p><code>find_element_by_*</code> 是一种定位网页元素的方法，有以下方式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">find_element_by_id</span><br><span class="line">find_element_by_name</span><br><span class="line">find_element_by_xpath</span><br><span class="line">find_element_by_link_text</span><br><span class="line">find_element_by_partial_link_text</span><br><span class="line">find_element_by_tag_name</span><br><span class="line">find_element_by_class_name</span><br><span class="line">find_element_by_css_selector</span><br></pre></td></tr></table></figure>

<p>可以通过标签的 id 属性、name 属性、class_name 属性查找元素，也可以通过 xpath 等，其中用到最多的就是 <code>xpath</code>。</p>
<p>例如查找百度的搜索框：在调试窗口中找到搜索框对应的标签，点击右键，选择 copy 下的 <code>copy xpath</code>，直接复制 xpath 。内容为：<code>//*[@id=&quot;kw&quot;]</code>，其实意思就是从根目录开始找，找到 id 属性为 kw 的标签。定位到搜索框，就可以输入想搜索的内容。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.keys <span class="keyword">import</span> Keys</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    driver = webdriver.Chrome(<span class="string">&quot;F:\学习\Python\chromedriver.exe&quot;</span>)</span><br><span class="line">    driver.get(<span class="string">&quot;https://www.baidu.com&quot;</span>)</span><br><span class="line">    elem = driver.find_element_by_xpath(<span class="string">&#x27;//*[@id=&quot;kw&quot;]&#x27;</span>)</span><br><span class="line">    elem.send_keys(<span class="string">&quot;剑来&quot;</span>)</span><br><span class="line">    elem.send_keys(Keys.RETURN)</span><br></pre></td></tr></table></figure>



<h3 id="Selenuim-的参考文档"><a href="#Selenuim-的参考文档" class="headerlink" title="Selenuim 的参考文档"></a>Selenuim 的参考文档</h3><blockquote>
<p>官方手册：<a target="_blank" rel="noopener" href="https://selenium-python.readthedocs.io/index.html">https://selenium-python.readthedocs.io/index.html</a></p>
<p>一篇参考文章：<a target="_blank" rel="noopener" href="https://blog.csdn.net/c406495762/article/details/72331737">https://blog.csdn.net/c406495762/article/details/72331737</a></p>
</blockquote>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p> 爬取小说、漫画、视频的详细内容可参考该链接的 <code>网络爬虫部分</code>：<a target="_blank" rel="noopener" href="https://github.com/Jack-Cherish/PythonPark">https://github.com/Jack-Cherish/PythonPark</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">小辑轻舟</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2022/09/15/python%E7%88%AC%E8%99%AB%E7%AC%94%E8%AE%B0/">http://example.com/2022/09/15/python爬虫笔记/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">知行记</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Python/">Python</a></div><div class="post_share"></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/images/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">小辑轻舟</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">9</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">9</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/97cz"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B"><span class="toc-number">1.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B%EF%BC%9A%E7%88%AC%E5%8F%96%E5%B0%8F%E8%AF%B4"><span class="toc-number">2.</span> <span class="toc-text">实例：爬取小说</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%88%AC%E5%8F%96%E7%BD%91%E9%A1%B5%E5%86%85%E5%AE%B9"><span class="toc-number">2.1.</span> <span class="toc-text">爬取网页内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E7%AB%A0%E8%8A%82%E9%93%BE%E6%8E%A5%E5%8F%8A%E7%AB%A0%E8%8A%82%E5%90%8D"><span class="toc-number">2.2.</span> <span class="toc-text">获取章节链接及章节名</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B4%E5%90%88%E4%BB%A3%E7%A0%81"><span class="toc-number">2.3.</span> <span class="toc-text">整合代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B%EF%BC%9A%E7%88%AC%E5%8F%96%E6%BC%AB%E7%94%BB"><span class="toc-number">3.</span> <span class="toc-text">实例：爬取漫画</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E7%AB%A0%E8%8A%82%E5%90%8D%E5%92%8C%E7%AB%A0%E8%8A%82%E9%93%BE%E6%8E%A5"><span class="toc-number">3.1.</span> <span class="toc-text">获取章节名和章节链接</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E6%BC%AB%E7%94%BB%E5%9B%BE%E7%89%87%E5%9C%B0%E5%9D%80"><span class="toc-number">3.2.</span> <span class="toc-text">获取漫画图片地址</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD%E5%9B%BE%E7%89%87"><span class="toc-number">3.3.</span> <span class="toc-text">下载图片</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B4%E5%90%88%E4%BB%A3%E7%A0%81-1"><span class="toc-number">3.4.</span> <span class="toc-text">整合代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B%EF%BC%9A%E7%88%AC%E5%8F%96%E8%A7%86%E9%A2%91"><span class="toc-number">4.</span> <span class="toc-text">实例：爬取视频</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9F%A5%E6%89%BE%E8%B5%84%E6%BA%90"><span class="toc-number">4.1.</span> <span class="toc-text">查找资源</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E6%AF%8F%E9%9B%86%E9%93%BE%E6%8E%A5"><span class="toc-number">4.2.</span> <span class="toc-text">获取每集链接</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E6%AF%8F%E9%9B%86%E8%A7%86%E9%A2%91%E5%AF%B9%E5%BA%94%E7%9A%84%E9%93%BE%E6%8E%A5"><span class="toc-number">4.3.</span> <span class="toc-text">获取每集视频对应的链接</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD%E8%A7%86%E9%A2%91"><span class="toc-number">4.4.</span> <span class="toc-text">下载视频</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B4%E5%90%88%E4%BB%A3%E7%A0%81-2"><span class="toc-number">4.5.</span> <span class="toc-text">整合代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%B9%E6%AE%8A%E6%83%85%E5%86%B5%E2%80%94%E2%80%94%E9%9D%9E%E7%9C%9F%E5%AE%9E%E9%93%BE%E6%8E%A5"><span class="toc-number">4.6.</span> <span class="toc-text">特殊情况——非真实链接</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7-Selenium"><span class="toc-number">5.</span> <span class="toc-text">自动化测试工具 Selenium</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%89%E8%A3%85Selenium%E5%8F%8A%E6%B5%8F%E8%A7%88%E5%99%A8%E9%A9%B1%E5%8A%A8"><span class="toc-number">5.1.</span> <span class="toc-text">安装Selenium及浏览器驱动</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B"><span class="toc-number">5.2.</span> <span class="toc-text">实例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#find-element-by"><span class="toc-number">5.3.</span> <span class="toc-text">find_element_by_*</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Selenuim-%E7%9A%84%E5%8F%82%E8%80%83%E6%96%87%E6%A1%A3"><span class="toc-number">5.4.</span> <span class="toc-text">Selenuim 的参考文档</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="toc-number">6.</span> <span class="toc-text">参考链接</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/01/31/Vue/" title="Vue"><img src="/images/(3).jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Vue"/></a><div class="content"><a class="title" href="/2023/01/31/Vue/" title="Vue">Vue</a><time datetime="2023-01-31T06:47:32.000Z" title="发表于 2023-01-31 14:47:32">2023-01-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/27/Mysql/" title="MySQL"><img src="/images/(9).jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="MySQL"/></a><div class="content"><a class="title" href="/2022/09/27/Mysql/" title="MySQL">MySQL</a><time datetime="2022-09-27T02:10:31.000Z" title="发表于 2022-09-27 10:10:31">2022-09-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/25/Mybatis/" title="Mybatis"><img src="/images/(3).jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Mybatis"/></a><div class="content"><a class="title" href="/2022/09/25/Mybatis/" title="Mybatis">Mybatis</a><time datetime="2022-09-25T11:41:56.000Z" title="发表于 2022-09-25 19:41:56">2022-09-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/24/SpringMVC/" title="SpringMVC"><img src="/images/(4).jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="SpringMVC"/></a><div class="content"><a class="title" href="/2022/09/24/SpringMVC/" title="SpringMVC">SpringMVC</a><time datetime="2022-09-24T07:25:03.000Z" title="发表于 2022-09-24 15:25:03">2022-09-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/22/Spring/" title="Spring"><img src="/images/(2).jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Spring"/></a><div class="content"><a class="title" href="/2022/09/22/Spring/" title="Spring">Spring</a><time datetime="2022-09-22T10:33:13.000Z" title="发表于 2022-09-22 18:33:13">2022-09-22</time></div></div></div></div></div></div></main><footer id="footer" style="background: rgb(116,116,116)"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023 By 小辑轻舟</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><p style="letter-spacing:2px; margin-top:0">平明拂剑朝天去，薄暮垂鞭醉酒归！<p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>